{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153e8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c3b460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.3.0.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.3.0.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importar la librería y las dependencias (todas las funciones nuevas están aquí)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks as cf\n",
    "cf.go_offline() \n",
    "\n",
    "import libreria as lb\n",
    "\n",
    "# Asumimos que X_limpio (atributos A1-A64) y Y (class) están disponibles después del Bloque 1\n",
    "# X_limpio debe estar limpio de nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebf55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4031e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 365, 'name': 'Polish Companies Bankruptcy', 'repository_url': 'https://archive.ics.uci.edu/dataset/365/polish+companies+bankruptcy+data', 'data_url': 'https://archive.ics.uci.edu/static/public/365/data.csv', 'abstract': 'The dataset is about bankruptcy prediction of Polish companies.The bankrupt companies were analyzed in the period 2000-2012, while the still operating companies were evaluated from 2007 to 2013.', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 10503, 'num_features': 65, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2016, 'last_updated': 'Sat Mar 09 2024', 'dataset_doi': '10.24432/C5F600', 'creators': ['Sebastian Tomczak'], 'intro_paper': {'ID': 417, 'type': 'NATIVE', 'title': 'Ensemble boosted trees with synthetic features generation in application to bankruptcy prediction', 'authors': 'Maciej Ziȩba, S. Tomczak, Jakub M. Tomczak', 'venue': 'Expert systems with applications', 'year': 2016, 'journal': None, 'DOI': '10.1016/j.eswa.2016.04.001', 'URL': 'https://www.semanticscholar.org/paper/c0b86d47505223db7e6085c10dd797169a32fa78', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The dataset is about bankruptcy prediction of Polish companies. The data was collected from Emerging Markets Information Service (EMIS, http://www.securities.com), which is a database containing information on emerging markets around the world. The bankrupt companies were analyzed in the period 2000-2012, while the still operating companies were evaluated from 2007 to 2013.\\r\\nBasing on the collected data five classification cases were distinguished, that depends on the forecasting period:\\r\\n- 1stYear â€“ the data contains financial rates from 1st year of the forecasting period and corresponding class label that indicates bankruptcy status after 5 years. The data contains 7027 instances (financial statements), 271 represents bankrupted companies, 6756 firms that did not bankrupt in the forecasting period.\\r\\n- 2ndYear â€“ the data contains financial rates from 2nd year of the forecasting period and corresponding class label that indicates bankruptcy status after 4 years. The data contains 10173 instances (financial statements), 400 represents bankrupted companies, 9773 firms that did not bankrupt in the forecasting period.\\r\\n- 3rdYear â€“ the data contains financial rates from 3rd year of the forecasting period and corresponding class label that indicates bankruptcy status after 3 years. The data contains 10503 instances (financial statements), 495 represents bankrupted companies, 10008 firms that did not bankrupt in the forecasting period.\\r\\n- 4thYear â€“ the data contains financial rates from 4th year of the forecasting period and corresponding class label that indicates bankruptcy status after 2 years. The data contains 9792 instances (financial statements), 515 represents bankrupted companies, 9277 firms that did not bankrupt in the forecasting period.\\r\\n- 5thYear â€“ the data contains financial rates from 5th year of the forecasting period and corresponding class label that indicates bankruptcy status after 1 year. The data contains 5910 instances (financial statements), 410 represents bankrupted companies, 5500 firms that did not bankrupt in the forecasting period.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'X1\\tnet profit / total assets\\r\\nX2\\ttotal liabilities / total assets\\r\\nX3\\tworking capital / total assets\\r\\nX4\\tcurrent assets / short-term liabilities\\r\\nX5\\t[(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] * 365\\r\\nX6\\tretained earnings / total assets\\r\\nX7\\tEBIT / total assets\\r\\nX8\\tbook value of equity / total liabilities\\r\\nX9\\tsales / total assets\\r\\nX10\\tequity / total assets\\r\\nX11\\t(gross profit + extraordinary items + financial expenses) / total assets\\r\\nX12\\tgross profit / short-term liabilities\\r\\nX13\\t(gross profit + depreciation) / sales\\r\\nX14\\t(gross profit + interest) / total assets\\r\\nX15\\t(total liabilities * 365) / (gross profit + depreciation)\\r\\nX16\\t(gross profit + depreciation) / total liabilities\\r\\nX17\\ttotal assets / total liabilities\\r\\nX18\\tgross profit / total assets\\r\\nX19\\tgross profit / sales\\r\\nX20\\t(inventory * 365) / sales\\r\\nX21\\tsales (n) / sales (n-1)\\r\\nX22\\tprofit on operating activities / total assets\\r\\nX23\\tnet profit / sales\\r\\nX24\\tgross profit (in 3 years) / total assets\\r\\nX25\\t(equity - share capital) / total assets\\r\\nX26\\t(net profit + depreciation) / total liabilities\\r\\nX27\\tprofit on operating activities / financial expenses\\r\\nX28\\tworking capital / fixed assets\\r\\nX29\\tlogarithm of total assets\\r\\nX30\\t(total liabilities - cash) / sales\\r\\nX31\\t(gross profit + interest) / sales\\r\\nX32\\t(current liabilities * 365) / cost of products sold\\r\\nX33\\toperating expenses / short-term liabilities\\r\\nX34\\toperating expenses / total liabilities\\r\\nX35\\tprofit on sales / total assets\\r\\nX36\\ttotal sales / total assets\\r\\nX37\\t(current assets - inventories) / long-term liabilities\\r\\nX38\\tconstant capital / total assets\\r\\nX39\\tprofit on sales / sales\\r\\nX40\\t(current assets - inventory - receivables) / short-term liabilities\\r\\nX41\\ttotal liabilities / ((profit on operating activities + depreciation) * (12/365))\\r\\nX42\\tprofit on operating activities / sales\\r\\nX43\\trotation receivables + inventory turnover in days\\r\\nX44\\t(receivables * 365) / sales\\r\\nX45\\tnet profit / inventory\\r\\nX46\\t(current assets - inventory) / short-term liabilities\\r\\nX47\\t(inventory * 365) / cost of products sold\\r\\nX48\\tEBITDA (profit on operating activities - depreciation) / total assets\\r\\nX49\\tEBITDA (profit on operating activities - depreciation) / sales\\r\\nX50\\tcurrent assets / total liabilities\\r\\nX51\\tshort-term liabilities / total assets\\r\\nX52\\t(short-term liabilities * 365) / cost of products sold)\\r\\nX53\\tequity / fixed assets\\r\\nX54\\tconstant capital / fixed assets\\r\\nX55\\tworking capital\\r\\nX56\\t(sales - cost of products sold) / sales\\r\\nX57\\t(current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)\\r\\nX58\\ttotal costs /total sales\\r\\nX59\\tlong-term liabilities / equity\\r\\nX60\\tsales / inventory\\r\\nX61\\tsales / receivables\\r\\nX62\\t(short-term liabilities *365) / sales\\r\\nX63\\tsales / short-term liabilities\\r\\nX64\\tsales / fixed assets', 'citation': None}}\n",
      "     name     role        type demographic description units missing_values\n",
      "0    year  Feature     Integer        None        None  None             no\n",
      "1      A1  Feature  Continuous        None        None  None             no\n",
      "2      A2  Feature  Continuous        None        None  None             no\n",
      "3      A3  Feature  Continuous        None        None  None             no\n",
      "4      A4  Feature  Continuous        None        None  None            yes\n",
      "..    ...      ...         ...         ...         ...   ...            ...\n",
      "61    A61  Feature  Continuous        None        None  None            yes\n",
      "62    A62  Feature  Continuous        None        None  None             no\n",
      "63    A63  Feature  Continuous        None        None  None            yes\n",
      "64    A64  Feature  Continuous        None        None  None            yes\n",
      "65  class   Target     Integer        None        None  None             no\n",
      "\n",
      "[66 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#IMPORTAMOS EL DATA SET DE ACUERDO A LA DOCUMENTACIÓN EN https://github.com/uci-ml-repo/ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "polish_companies_bankruptcy = fetch_ucirepo(id=365) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = polish_companies_bankruptcy.data.features \n",
    "y = polish_companies_bankruptcy.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(polish_companies_bankruptcy.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(polish_companies_bankruptcy.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dc6bd",
   "metadata": {},
   "source": [
    "Explicación: El Análisis Exploratorio de Datos (EDA) es el primer paso para entender las características principales y los problemas del conjunto de datos. La limpieza es crucial, ya que los datos de baja calidad pueden afectar directamente el rendimiento de los modelos (GIGO: Garbage in -> Garbage out). En este caso, trataremos los valores faltantes (nulos) y exploraremos las distribuciones para identificar outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 2.1. Análisis de Valores Faltantes ---\")\n",
    "\n",
    "# 1. Revisar la completitud inicial [5]\n",
    "print(\"Porcentaje de nulos antes de la imputación:\")\n",
    "print(completitud_datos(X))\n",
    "\n",
    "# 2. Imputación de nulos\n",
    "# Todas las variables A1-A64 son de tipo float64 (continuas) [33].\n",
    "X_limpio = limpiar_nulos_imputer(X, \n",
    "                                 continuous_cols=columnas_financieras, \n",
    "                                 discrete_cols=[])\n",
    "\n",
    "print(\"\\nPorcentaje de nulos después de la imputación:\")\n",
    "print(completitud_datos(X_limpio))\n",
    "\n",
    "print(\"\\n--- 2.2. Análisis Descriptivo y Outliers (EDA Visual)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2be5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 2.2. Análisis Descriptivo y Outliers (EDA Visual) ---\")\n",
    "\n",
    "# 3. Visualizar distribuciones para encontrar outliers y sesgos [28]\n",
    "# Tomamos una muestra de columnas para no saturar la salida\n",
    "columnas_muestra_eda = ['A1', 'A5', 'A9', 'A10']\n",
    "generar_histograma_cf(X_limpio, columnas_muestra_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000215b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Uso de Boxplots para detección visual de Outliers\n",
    "generar_boxplot_cf(X_limpio, columnas_muestra_eda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Tratamiento de un Outlier (Ejemplo para A5, que tiene alta Std Dev [36])\n",
    "# El tratamiento de outliers es importante ya que pueden tener una fuerte influencia en los modelos estadísticos [37].\n",
    "X_filtrado_a5 = tratar_outliers_iqr(X_limpio, 'A5', factor=1.5)\n",
    "print(f\"\\nTamaño original (después de imputar): {len(X_limpio)}\")\n",
    "print(f\"Tamaño después de filtrar outliers en A5: {len(X_filtrado_a5)}\")\n",
    "\n",
    "# Usaremos X_limpio para los siguientes pasos, ya que PCA y VarClusHi a menudo prefieren la imputación al descarte total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2be339",
   "metadata": {},
   "source": [
    "[Bloque 3: Multicolinealidad y Reducción de Dimensionalidad]\n",
    "Explicación: Debido a la alta dimensionalidad (64 variables) y la fuerte correlación detectada entre pares de variables (algunas cercanas a 1.0000), es esencial aplicar técnicas de reducción. Esto mejora la interpretación, reduce la redundancia y evita la maldición de la dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1. Detección de Multicolinealidad con VarClusHi\n",
    "print(\"\\n--- 3.1. Detección de Multicolinealidad con VarClusHi ---\")\n",
    "\n",
    "# 1. Escalar los datos antes de VarClusHi (aunque VarClusHi no siempre lo requiere, es buena práctica)\n",
    "# Usaremos Standard Scaler [12].\n",
    "X_escalado = escalar_datos(X_limpio, method='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb50fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ejecutar VarClusHi\n",
    "# VarClusHi agrupa variables altamente correlacionadas [20].\n",
    "resultados_vc = analisis_varclushi(X_escalado)\n",
    "\n",
    "print(\"\\nResultados de VarClusHi (Top 10):\")\n",
    "print(resultados_vc.head(10))\n",
    "\n",
    "# **Justificación de selección de representante:**\n",
    "# Las variables que tienen el 'RS_Ratio' más bajo dentro de su clúster son \n",
    "# las mejores representantes, ya que implican una mayor correlación con \n",
    "# el componente principal del clúster y mayor diferencia con otros clústeres [14, 23]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.2. Reducción de Dimensiones con PCA\n",
    "print(\"\\n--- 3.2. Reducción con PCA ---\")\n",
    "\n",
    "# 1. Aplicar PCA para retener el 90% de la varianza total [16, 17]\n",
    "# PCA encuentra las direcciones (componentes) que contienen la máxima varianza [14].\n",
    "Xp_90 = analisis_pca(X_escalado, n_components=0.90)\n",
    "\n",
    "print(f\"\\nNúmero de componentes para el 90% de varianza: {Xp_90.shape[19]}\")\n",
    "print(Xp_90.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Aplicar PCA para obtener solo 3 componentes (para visualización 3D)\n",
    "Xp_3 = analisis_pca(X_escalado, n_components=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1850d6b",
   "metadata": {},
   "source": [
    "[Bloque 4: Visualización de Componentes PCA]\n",
    "Explicación: La visualización es clave para transformar datos en conocimiento. Al reducir el espacio de 64 dimensiones a 2 o 3 componentes, podemos graficar la estructura interna de los datos y observar posibles agrupaciones (clústeres) o outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e59108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 4.1. Visualización de PCA (2D) ---\")\n",
    "# Visualización 2D de los dos primeros componentes\n",
    "visualizar_pca_componentes(Xp_3.iloc[:, :2], dim=2) \n",
    "\n",
    "print(\"\\n--- 4.2. Visualización de PCA (3D) ---\")\n",
    "# Visualización 3D (requiere PC1, PC2, PC3) [44]\n",
    "visualizar_pca_componentes(Xp_3, dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bafceb",
   "metadata": {},
   "source": [
    "Bloque 5: Selección de Variables Supervisada (SelectKBest)\n",
    "Explicación: La Selección de Variables Predictivas es un método supervisado (depende de la variable objetivo, Y) para combatir la Maldición de la Dimensionalidad y mejorar la interpretabilidad del modelo. SelectKBest utiliza pruebas estadísticas (como F-classif para clasificación) para asignar una puntuación a cada variable, priorizando aquellas que tienen la asociación más fuerte con la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 5.1. Aplicación de SelectKBest ---\")\n",
    "\n",
    "# La variable objetivo Y (class) es binaria (0/1), lo que requiere F_classif o Chi2 [4].\n",
    "# Usaremos F_classif para datos continuos.\n",
    "# X_escalado proviene del Bloque 3 (data limpia y estandarizada).\n",
    "Y = df_polish['class'] # Variable objetivo binaria (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb340d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SelectKBest para seleccionar las 7 mejores variables por defecto\n",
    "# Nota: La función SelectKBest requiere que X contenga valores no negativos para chi2 [5].\n",
    "# Usaremos f_classif que funciona bien con datos escalados (StandardScaler).\n",
    "X_kbest_7, scores_kbest = seleccionar_kbest(X_escalado, Y, k=7, score_func='f_classif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLas 7 mejores variables seleccionadas son:\")\n",
    "print(X_kbest_7.columns.tolist())\n",
    "\n",
    "# Visualizar el ranking de todas las variables (Poder Predictivo)\n",
    "# scores_kbest.set_index('Variable')['Puntuación'].iplot(kind='bar', title='Puntuación KBest por Variable')\n",
    "\n",
    "# Justificación: Solo se conservan las 7 variables que estadísticamente son más predictivas de Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b11e6d",
   "metadata": {},
   "source": [
    "Bloque 6: Transformación Entrópica (WoE y IV)\n",
    "Explicación: El Peso de la Evidencia (WoE) y el Valor de Información (IV) son herramientas cruciales en la modelización de clasificación binaria (como el riesgo crediticio o la bancarrota). El WoE transforma las categorías de una variable en una nueva escala numérica basada en el logaritmo de las probabilidades de Eventos (bancarrota) vs No Eventos (no bancarrota). El IV resultante cuantifica el poder predictivo de esa variable.\n",
    "6.1. Conceptos Teóricos\n",
    "• Peso de la Evidencia (WoE): Indica si una categoría tiene una mayor o menor proporción de eventos (ej. bancarrota) que la población total. Si WoE > 0, el grupo tiene más No Eventos ('Buenos'); si WoE < 0, tiene más Eventos ('Malos').\n",
    "• Valor de Información (IV): Mide el poder predictivo total. Un IV alto (0.3 a 0.5) es considerado un predictor fuerte; valores bajos (< 0.02) son inútiles.\n",
    "• Requisito: Las variables continuas (A1-A64) deben ser discretizadas (binning) antes de aplicar WoE/IV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 6.2. Transformación Entrópica (IV y WoE) ---\")\n",
    "\n",
    "# Usaremos la variable A1 (activos / total de pasivos) como ejemplo.\n",
    "FEATURE_NAME = 'A1' \n",
    "\n",
    "# 1. Discretización de la variable continua (binning) [7]\n",
    "# Creamos 5 bins basados en cuantiles.\n",
    "A1_binned = discretizar_variable_para_woe(X_limpio, feature=FEATURE_NAME, n_bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame temporal con la variable binned y el target\n",
    "df_woe = pd.DataFrame({'A1_binned': A1_binned, 'class': Y})\n",
    "\n",
    "# 2. Calcular WoE e IV\n",
    "# Calculamos WoE por cada rango de A1 y el IV total para la variable A1\n",
    "resultados_woe_iv, iv_total = calcular_woe_iv_variable(\n",
    "    df_woe, \n",
    "    feature='A1_binned', \n",
    "    target='class'\n",
    ")\n",
    "\n",
    "print(f\"\\nResultados WoE y IV para la variable {FEATURE_NAME} (Discretizada en 5 bins):\")\n",
    "print(resultados_woe_iv)\n",
    "print(f\"\\nValor de Información (IV) total para {FEATURE_NAME}: {iv_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22624398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del Poder Predictivo:\n",
    "if iv_total < 0.02:\n",
    "    print(\"IV sugiere que la variable es impredecible / inútil [9].\")\n",
    "elif 0.1 <= iv_total <= 0.3:\n",
    "    print(\"IV sugiere que la variable es un predictor medio [9].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225d9f7",
   "metadata": {},
   "source": [
    "El proceso que hemos completado es similar a la labor de un cartógrafo que intenta representar un planeta complejo (el dataset de 64 dimensiones) en un mapa plano (2D o 3D). Primero, el cartógrafo debe limpiar el territorio (imputar nulos y tratar outliers). Luego, utiliza PCA para encontrar los ejes de rotación más significativos del planeta (los Componentes Principales), permitiéndonos comprimir la inmensa cantidad de información original en un dibujo simple pero representativo, facilitando la toma de decisiones y la identificación de regiones clave (patrones o grupos de empresas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
